import openai  # Imports the OpenAI library for interacting with OpenAI's APIs
import lmql  # Imports the LMQL library for querying language models with logical constraints
import asyncio  # Imports the asyncio library for writing asynchronous code
import nest_asyncio  # Imports the nest_asyncio library to allow nested use of the asyncio event loop
from lmql.lib.chat import message
# Imports the 'message' class from the 'chat' module in the 'lmql.lib' package

nest_asyncio.apply()  
# Applies the nest_asyncio patch to allow the current event loop to run nested event loops

# Topic Interaction - Prompt Construction and Constrained Text Generation
# The following program instructs the model to choose a value from a set of numbers that corresponds to
# many U.S. Presidents that there have been.
# The program employs multi-part prompting by instructing the prompt to use chain-of-thought reasoning 
# while generating its answer.  
# The program also uses a set constraint to ensure that the model selects its answer from a given set of values. 
# The program output is the model's step-by-step reasoning, as well as its final answer. 

# Example output: 
# REASONING: The United States has had a total of 46 presidents since its founding in 1776.
# This number includes both elected and acting presidents, as well as those who served multiple terms.
# Therefore, there have been 46 U.S. presidents in total. 
# ANSWER: 46

@lmql.query # Marks this function as an LMQL query definition
def multi_part_text_gen(): # Defines a function within the LMQL query
    '''lmql # Indicates that the following block uses LMQL syntax
    "Q: How many U.S. Presidents have there been?"  # Question for the model to answer
    "A: Let's think step by step about the answer. Your reasoning should only be 3 sentences.\n"  
    # Instruction for reasoning format
    "[REASONING]"  
    # Placeholder for the model's step-by-step reasoning
    "what is the final answer\n\n"  
    # Requests the model to provide the final answer
    "[ANSWER]" where ANSWER in set(["25", "46", "83", "40", "15", "60"])  
    # Constrains the final answer to a specific set of values
    return REASONING, ANSWER  # Outputs both reasoning and final answer 
    '''
print(multi_part_text_gen())
# Calls the multi_part_text_gen query and prints its output

# Topic Interaction - Prompt construction, Constraints, and Tool Augmentation
# The following LMQL query function instructs the model to output a list of ingredients needed
# to make a given food that is input by the user. 
# In addition to the prompt construction exhibited in the top-level query and the Python control flow for loop, 
# the program uses the STOPS_AT constraint to ensure there are no newline characters in the recipe output. 
# The program also uses tool augmentation with the append() and strip() methods to strip ingredients
# of white space and add each generated ingredient to the recipe. 
# The program output is the generated list of inrgedients, aka the recipe for a given food. 

# Example Input:
# Pancke
# Example Output: 
# ['1 cup all-purpose flour', '2 tablespoons sugar', '2 teaspoons baking powder', 
# '1/2 teaspoon salt', '1 cup milk']

@lmql.query  # Marks this function as an LMQL query definition
def recipe(food):  # Defines a function that accepts an "event" parameter
    '''lmql # Indicates that the following block is written in LMQL
    "Q: Output a list of ingredients needed to make {food}. \n" 
    # Internal prompt statement, dynamically inserting the 'food' into the question w/ string interpolation
    recipe = []  # Initializes an empty Python list to store the generated ingredients
    for i in range(5): # Loops 5 times to collect 5 ingredients
        "-[INGREDIENT]" where STOPS_AT(INGREDIENT, "\n")  
        # Queries the model for an ingredient and ensures it stops at a newline character
        recipe.append(INGREDIENT.strip())  
        # Strips each ingredient of white space and appends it to the list
    return recipe  # Returns the recipe of ingredients generated by the model
    '''
print(recipe("pancakes")) 
# Calls the recipe query with "pancakes" as the food and prints the result

# Topic Interaction - Prompt construction, Constraints, and Tool Augmentation
# The following LMQL query function instructs the model to output a list of dates, then indicate
# which date in this list occurs earliest in the year. 
# The prompt construction and tool augmentation employes in this program are very similar to the above program
# (top-level queries, Python control flow, append() and strip() methods).
# However, in this program the text generation constraint differs, and REGEX contraints are used to ensure 
# that each date generated is output in MM/DD format. 
# The output of the program is the generated list of dates, as well as which date occurs earliest in the year.


@lmql.query()  # Marks this function as an LMQL query definition, enabling interaction with an LMQL model
def calendar():  # Defines the function that contains the LMQL query
    '''lmql  # Indicates that the following block is written in LMQL syntax
    "Q: Output a list of dates. \n" 
    # Internal prompt statement asking the model to output a list of dates
    dates = [] # Initializes an empty Python list to store the generated dates
    for i in range(4): # Loops 4 times to collect 4 dates
        "[DATE]" where REGEX(DATE, r"[0-9]{2}/[0-9]{2}") 
         # Queries the model for a date and uses a REGEX constraint to ensure the date is in DD/MM format
        dates.append(DATE.strip()) # Strips each date of white space and appends it to the list of dates
    "From this generated list of dates, which one is earliest in the year?"
    # Internal prompt asking the model to output which date in the list of dates occurs earliest in the year
    "[EARLYDATE]" where REGEX(EARLYDATE, r"[0-9]{2}/[0-9]{2}")
    # Returns the earlist date with a REGEX constraint ensuring that it is in DD/MM format
    return dates, EARLYDATE  # Returns the list of dates generated by the model and the earliest date
    '''
print(calendar())  
# Calls the regex_constraints query and prints its output

# Topic Interaction - Chatbot and Text Generation Constraints
# The following LMQL query implements the same custom, interactive chatbot as seen in the chatbot section. 
# However, within this program the STOPS_AT constraint is employed when generating the chatbot assistant messages
# such that the chatbot's assistant messages stop when they generate a ".", or after the first sentence. 


# Example Chatbot Interaction:
# User:
# Explain why rainbows are curved in three sentences
# Assistant:
# Rainbows are curved because of the way light is refracted and reflected inside raindrops.

@lmql.query()  # Marks the function as an LMQL query
def one_sentence_chatbot():  # Defines a chatbot function
    '''lmql # Indicates that the following block is written in LMQL
    print("Chatbot is ready. Type 'Thanks! Goodbye' to exit.")  
    # Prints a welcome message for the chatbot
    argmax  # Starts the query definition 
        "{:system} You are a chatbot serving highly educated people. your answers can be complicated."  
        # Sets system instructions for the chatbot on how to answer questions
        while True:  # Begins an infinite loop for chatbot interaction
            print("User:")  # Prompts the user for input
            "{:user} {await input()}"  # Takes user input asynchronously
            "{:assistant} Think step by step to develop your answer:[REASONING]"  
            # Prompts assistant to reason through the answer
            "{:assistant} External Answer: [@message ANSWER]" where STOPS_AT(ANSWER, ".")
            # Sends the reasoning to the message service to generate an answer,
            # also adding the contrainst that the answer should stop after the first sentence
            if "Goodbye!" in ANSWER:  # Breaks the loop if the user says "Goodbye!"
                break
            print("Assistant:")  # Prints text to indicate the following response is the assistant's
            print(ANSWER)  # Prints the generated answer
    from "chatgpt"  # Pulls a response from a ChatGPT-based model
    '''

result = one_sentence_chatbot()  # Calls the chatbot function
print(result.variables["ANSWER"]) 